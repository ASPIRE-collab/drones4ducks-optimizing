{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run inference in tiles without having to save tiled images separately using Sahi\n",
    "#Based off of: https://colab.research.google.com/github/obss/sahi/blob/main/demo/inference_for_yolov5.ipynb?v=1#scrollTo=yXyP3T0wyGtF\n",
    "\n",
    "#Import necessary modules\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "from sahi.utils.yolov5 import (\n",
    "    download_yolov5s6_model,\n",
    ")\n",
    "\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.utils.cv import read_image\n",
    "from sahi.utils.file import download_from_url\n",
    "from sahi.predict import get_prediction, get_sliced_prediction, predict\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set detection model parameters\n",
    "\n",
    "yolov5_model_path = 'models/best.pt' \n",
    "download_yolov5s6_model(destination_path=yolov5_model_path)\n",
    "\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type = 'yolov5', #works with different architectures, for future reference\n",
    "    model_path = yolov5_model_path,\n",
    "    confidence_threshold = 0.3, #Can specify according to need\n",
    "    device = \"cuda:0\" #to use GPU, can also use CPU but why would you?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run test inference and visualize result on a single image; save image output\n",
    "path = \"\" #Image directory\n",
    "img = \"name.JPG\" #example image\n",
    "image = path+img\n",
    "\n",
    "result = get_sliced_prediction(\n",
    "    image,\n",
    "    detection_model,\n",
    "    slice_height = 456,\n",
    "    slice_width = 781,\n",
    "    overlap_height_ratio = 0.1,\n",
    "    overlap_width_ratio = 0.1\n",
    ")\n",
    "\n",
    "#Save as COCO annotations-- add image ID to annotation\n",
    "coco = result.to_coco_annotations()\n",
    "for annotation in coco:\n",
    "    annotation['image_id'] = img\n",
    "\n",
    "#Display detection output\n",
    "result.export_visuals(export_dir=\"\") #Specify directory\n",
    "Image(export_dir+\"prediction_visual.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set parameters for batch prediction\n",
    "\n",
    "model_type = \"yolov5\"\n",
    "model_path = yolov5_model_path\n",
    "model_device = 'cuda:0' # to use GPU\n",
    "model_confidence_threshold = 0.3 #specify according to need\n",
    "\n",
    "slice_height = 456 #specify according to need\n",
    "slice_width = 781 #specify according to need\n",
    "overlap_height_ratio = 0.1 #specify according to need\n",
    "overlap_width_ratio = 0.1 #specify according to need\n",
    "\n",
    "source_image_dir = \"\" #Specify image directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Currently this only outputs images, no text output of annotations; need to fix it\n",
    "result = predict(\n",
    "    model_type=model_type,\n",
    "    model_path=model_path,\n",
    "    model_device=model_device,\n",
    "    model_confidence_threshold=model_confidence_threshold,\n",
    "    source=source_image_dir,\n",
    "    slice_height=slice_height,\n",
    "    slice_width=slice_width,\n",
    "    overlap_height_ratio=overlap_height_ratio,\n",
    "    overlap_width_ratio=overlap_width_ratio,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
