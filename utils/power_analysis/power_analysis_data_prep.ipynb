{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power Analysis Data Prep Sequence\n",
    "Rowan Converse, last update 10/17/2023\n",
    "\n",
    "Requirements: \n",
    "Drone image directory\n",
    "A GIS software or Agisoft Metashape\n",
    "DJI Drones only: EXIF CSV output from EXIFTOOL\n",
    "\n",
    "Manual steps will be noted in markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: Clip circles (detection radius) from all images; preserve original image metadata\n",
    "\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def clip_circle(input_image_path, output_dir):\n",
    "    # Open the input image\n",
    "    image = Image.open(input_image_path)\n",
    "\n",
    "    # Get image dimensions\n",
    "    width, height = image.size\n",
    "\n",
    "    # Calculate the center of the image\n",
    "    center_x = width // 2\n",
    "    center_y = height // 2\n",
    "\n",
    "    # Calculate the radius as the shortest distance to the image edge\n",
    "    radius = min(center_x, center_y)\n",
    "\n",
    "    # Create a mask to represent the circular clip\n",
    "    mask = Image.new('L', (width, height), 0)\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "    draw.ellipse((center_x - radius, center_y - radius, center_x + radius, center_y + radius), fill=255)\n",
    "\n",
    "    # Apply the circular mask to the image\n",
    "    result = Image.new('RGB', (width, height))\n",
    "    result.paste(image, mask=mask)\n",
    "\n",
    "    # Preserve EXIF data\n",
    "    result.info = image.info\n",
    "\n",
    "    # Construct the output image name\n",
    "    output_image_name = os.path.splitext(os.path.basename(input_image_path))[0] + \".jpg\"\n",
    "    output_image_path = os.path.join(output_dir, output_image_name)\n",
    "\n",
    "    # Save the result with preserved EXIF data\n",
    "    result.save(output_image_path, exif=result.info['exif'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = \"E:\\\\poweranalysis\\\\data\\\\originals\\\\17b\\\\\"\n",
    "    output_dir = \"E:\\\\poweranalysis\\\\data\\\\circles\\\\17b\\\\\"\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # List all files in the input directory\n",
    "    image_files = [f for f in os.listdir(input_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif'))]\n",
    "\n",
    "    for image_file in image_files:\n",
    "        input_image_path = os.path.join(input_dir, image_file)\n",
    "        clip_circle(input_image_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual step: derive transect IDs\n",
    "In Agisoft, add the images and align them\n",
    "Alternately, in a GIS, add the EXIF csv and add the image centers as XY data\n",
    "Add a new column for transect ID to the output of step 2\n",
    "Manually note the image names of the start and end of each flightline; use this to designate a transect ID for all images\n",
    "\n",
    "Use an image select script to separate out the circle clips into folders based off transects\n",
    "With the known side overlap (flightplan, or manually derived from Agisoft footprints_to_shapes.py + GIS overlap of sample image footprints), preserve the appropriate number of independent side-lap transects before moving on to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a- Derive image metadata (AGL + GSD): DJI drones \n",
    "\n",
    "#Using a csv output from ExifTool that contains Relative Altitude (DJI drones only, so far)\n",
    "import csv\n",
    "# Define sensor parameters (in millimeters)-- this is Mavic 2 Pro\n",
    "image_width = 5472  # Example image width in pixels\n",
    "image_height = 3648  # Example image height in pixels\n",
    "sensor_width = 0.0128  # Make sure this is converted to meters!\n",
    "sensor_height = 0.0096  # Make sure this is converted to meters!\n",
    "focal_length = 0.01035  # Make sure this is converted to meters!\n",
    "\n",
    "altitude_data = []  # List to store extracted altitude values\n",
    "\n",
    "#Requires CSV output from ExifTool\n",
    "with open('E:\\\\poweranalysis\\\\data\\\\circles\\\\RGNC\\\\exif.csv', mode='r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        # Extract the \"RelativeAltitude\" and \"FileName\" values\n",
    "        relative_altitude = row.get(\"RelativeAltitude\", None)\n",
    "        image_filename = row.get(\"FileName\", None)\n",
    "\n",
    "        if relative_altitude is not None and relative_altitude.strip() != '' and image_filename is not None:\n",
    "            try:\n",
    "                relative_altitude = float(relative_altitude)\n",
    "                if relative_altitude > 120:\n",
    "                    # Flag values exceeding 120 as likely absolute altitudes\n",
    "                    print(f\"Skipped: {image_filename} - RelativeAltitude exceeds legal flight ceiling: {relative_altitude}\")\n",
    "                else:\n",
    "                    altitude_data.append((image_filename, relative_altitude, None))\n",
    "            except ValueError:\n",
    "                print(f\"Skipping: {image_filename} - Invalid RelativeAltitude value: {relative_altitude}\")\n",
    "\n",
    "# Calculate GSD and write data to a new CSV file\n",
    "with open('E:\\\\poweranalysis\\\\data\\\\circles\\\\RGNC\\\\RGNC_gsd.csv', mode='w', newline='') as csv_output:\n",
    "    csv_writer = csv.writer(csv_output)\n",
    "    csv_writer.writerow([\"filename\", \"agl\", \"gsd\"])\n",
    "    \n",
    "    for image_filename, relative_altitude in altitude_data:\n",
    "        # Calculate GSD (in cm)\n",
    "        gsd_m = (sensor_width * relative_altitude) / (focal_length * image_width)\n",
    "        gsd_cm = gsd_m * 100  # Convert to cm\n",
    "\n",
    "        # Write data to the new CSV file\n",
    "        csv_writer.writerow([image_filename, relative_altitude, gsd_cm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2b: Derive AGL/GSD, all other drones\n",
    "#Assuming you have a CSV file with EXIF data that has absolute altitude\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "ground_elevation = 1371 #Adjust to the appropriate value for your study area; I typically use the recorded flight absolute altitude of the first image minus the idealized AGL programmed for the flight\n",
    "\n",
    "# Define sensor parameters\n",
    "#SONY RXR1 II\n",
    "image_width = 7952  # Example image width in pixels\n",
    "image_height = 5304  # Example image height in pixels\n",
    "sensor_width = 0.0359   # Make sure this is converted to meters!\n",
    "sensor_height = 0.024  # Make sure this is converted to meters!\n",
    "focal_length = 0.035 # Make sure this is converted to meters!\n",
    "\n",
    "#Mavic 2 Pro\n",
    "#image_width = 5472  # Example image width in pixels\n",
    "#image_height = 3648  # Example image height in pixels\n",
    "#sensor_width = 0.0128  # Make sure this is converted to meters!\n",
    "#sensor_height = 0.0096  # Make sure this is converted to meters!\n",
    "#focal_length = 0.01035  # Make sure this is converted to meters!\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"E:\\\\poweranalysis\\\\data\\\\circles\\\\40_ind_sides.csv\")\n",
    "\n",
    "# Calculate and store GSD values in a new column\n",
    "df['gsd'] = (sensor_width * (df['agl'] - ground_elevation)) / (focal_length * image_width)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(\"E:\\\\poweranalysis\\\\data\\\\circles\\\\40_gsd.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3a: Derive independent forward overlap images (must use a known overlap value)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a pandas dataframe\n",
    "input_file = 'E:\\\\poweranalysis\\\\data\\\\circles\\\\40_gsd.csv'  # CSV file that includes image names and transect numbers, ideally filtered by desired transects\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Create a new dataframe with every other row from the original dataframe; can replace \"2\" with any n value of images to preserve\n",
    "subset_df = df.iloc[::2].copy()\n",
    "\n",
    "# Optional: Reset the index of the new dataframe\n",
    "subset_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the subset dataframe to a new CSV file\n",
    "output_file = 'E:\\\\poweranalysis\\\\data\\\\circles\\\\40_ind_img.csv'  # Replace with the desired filename\n",
    "subset_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3b: concatenate the different sites, and copy the independent images into a common directory for detection step\n",
    "\n",
    "# List of CSV file paths to merge\n",
    "csv_files = ['E:\\\\poweranalysis\\\\data\\\\circles\\\\6_ind_img.csv', 'E:\\\\poweranalysis\\\\data\\\\circles\\\\17a_ind_img.csv', 'E:\\\\poweranalysis\\\\data\\\\circles\\\\17b_ind_img.csv', 'E:\\\\poweranalysis\\\\data\\\\circles\\\\40_ind_img.csv']  # Add your file paths\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each CSV file and append it to the merged DataFrame\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('E:\\\\poweranalysis\\\\data\\\\circles\\\\BdA_2023_ind_imgs.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3c- Copy the independent images into a new directory for batch detection\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Directory where your images are stored\n",
    "image_directory = 'E:\\\\poweranalysis\\\\data\\\\circles\\\\all_circles'\n",
    "\n",
    "# Directory where you want to copy the matching images\n",
    "output_directory = 'E:\\\\poweranalysis\\\\data\\\\circles\\\\BdA2023_detection'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Read the merged CSV file\n",
    "df = pd.read_csv('E:\\\\poweranalysis\\\\data\\\\circles\\\\BdA_2023_ind_imgs.csv')\n",
    "\n",
    "# Create a set of all filenames from the CSV in lowercase for case-insensitive matching\n",
    "csv_filenames = set(merged_df['filename'].str.lower())\n",
    "\n",
    "# Walk through the image directory and its subdirectories\n",
    "for root, dirs, files in os.walk(image_directory):\n",
    "    for file in files:\n",
    "        if file.lower() in csv_filenames:\n",
    "            source_path = os.path.join(root, file)\n",
    "            target_path = os.path.join(output_directory, file)\n",
    "            shutil.copy(source_path, target_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MANUAL STEP: Land/no land\n",
    "2 ways to do this\n",
    "\n",
    "1- If there are a lot of images: Build an orthomosaic in Agisoft with all images. Then project the ortho in a GIS, digitize a boundary of the shoreline. Project the image center XY data. Derive an average image height using the average GSD. Using this info, create an interior buffer from the polygon using that measurement. Select all image centers within the buffer. Export this information-- this is your \"no land\" set\n",
    "\n",
    "2- If there aren't a lot of images: just examine the circle clip thumbnails in the Windows Explorer and note any with land in the CSV manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2023-7-27 Python-3.10.9 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 214 layers, 7027720 parameters, 0 gradients, 16.0 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n",
      "Performing prediction on 140 number of slices.\n"
     ]
    }
   ],
   "source": [
    "#4: Batch tiled detections using SAHI tools\n",
    "\n",
    "import os\n",
    "from sahi.utils.yolov5 import download_yolov5s6_model\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.utils.cv import read_image\n",
    "from sahi.utils.file import download_from_url\n",
    "from sahi.predict import get_prediction, get_sliced_prediction, predict\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "\n",
    "def process_images_in_directory(master_directory, save_directory):\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    yolov5_model_path = 'E:\\\\training\\\\Base\\\\YOLOweights\\\\zooniverse_3class_150epc\\\\weights\\\\best.pt' # specify the full model path\n",
    "    download_yolov5s6_model(destination_path=yolov5_model_path)\n",
    "\n",
    "    detection_model = AutoDetectionModel.from_pretrained(\n",
    "        model_type='yolov5', # can accommodate other architectures\n",
    "        model_path=yolov5_model_path,\n",
    "        confidence_threshold=0.8, # specify the confidence threshold as desired\n",
    "        device=\"cuda:0\" # to use GPU\n",
    "    )\n",
    "\n",
    "    all_anns = []\n",
    "\n",
    "    for root, dirs, files in os.walk(master_directory):\n",
    "        for img_filename in files:\n",
    "            if img_filename.lower().endswith(('.tif', '.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                img_path = os.path.join(root, img_filename)\n",
    "\n",
    "                result = get_sliced_prediction(\n",
    "                    img_path,\n",
    "                    detection_model,\n",
    "                    slice_height=640, # Can adjust tile size\n",
    "                    slice_width=640,\n",
    "                    overlap_height_ratio=0.1, # Can adjust tile overlap\n",
    "                    overlap_width_ratio=0.1\n",
    "                )\n",
    "\n",
    "                coco = result.to_coco_annotations()\n",
    "                for annotation in coco:\n",
    "                    annotation['image_id'] = img_filename\n",
    "\n",
    "                all_anns.extend(coco)\n",
    "\n",
    "    df = pd.DataFrame(all_anns)\n",
    "    savepath = os.path.join(save_directory, \"BdA2023_detections.csv\") # change the name if desired\n",
    "    df.to_csv(savepath, index=False)\n",
    "\n",
    "image_directory = \"E:\\\\poweranalysis\\\\data\\\\circles\\\\BdA2023_detection\"  # Replace with the path to the master directory containing images\n",
    "save_directory = \"E:\\\\poweranalysis\\\\data\\\\circles\"  # Replace with the desired output directory for the COCO annotations\n",
    "process_images_in_directory(image_directory, save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5- Distance from bird detections to the drone\n",
    "\n",
    "#Fixed to use altitude/GSD from each individual image to calculate ground distance from drone of individual annotations\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Read the two CSV files\n",
    "df_annotations = pd.read_csv(\"E:\\\\poweranalysis\\\\data\\\\circles\\\\BdA2023_detections.csv\") #COCO format SAHI outputs\n",
    "df_annotations['bbox'] = df_annotations['bbox'].apply(ast.literal_eval) #fix the way bounding box is read\n",
    "df_resolution = pd.read_csv(\"E:\\\\poweranalysis\\\\data\\\\circles\\\\BdA_2023_ind_imgs.csv\") # output from step 2\n",
    "def lowercase_extension(filename):\n",
    "    name, ext = filename.rsplit('.', 1)\n",
    "    return f'{name}.{ext.lower()}'\n",
    "\n",
    "# Lowercase the extension part of filenames in one of the dataframes\n",
    "df_resolution['filename'] = df_resolution['filename'].apply(lowercase_extension)\n",
    "\n",
    "# Merge the two dataframes based on the 'filename' column\n",
    "merged_df = df_annotations.merge(df_resolution, on='filename', how='inner')\n",
    "\n",
    "# Image center in pixels\n",
    "center_x_px, center_y_px = 3996, 2652  #IMPORTANT: Specify according to your sensor (1/2 width/height). This is for Sony RX1R II (Wingtra)\n",
    "\n",
    "# Calculate the center point in meters\n",
    "merged_df['center_x_m'] = center_x_px * merged_df['gsd']\n",
    "merged_df['center_y_m'] = center_y_px * merged_df['gsd']\n",
    "\n",
    "# Function to calculate distance from center\n",
    "def calculate_distance_from_center(row):\n",
    "    # Get the coordinates of the bounding box (x, y, width, height)\n",
    "    x, y, width, height = row['bbox']\n",
    "\n",
    "    # Calculate the center point of the bounding box in pixels\n",
    "    bbox_center_x_px = x + (width / 2)\n",
    "    bbox_center_y_px = y + (height / 2)\n",
    "\n",
    "    # Calculate the center point of the bounding box in meters\n",
    "    bbox_center_x_m = bbox_center_x_px * row['gsd']\n",
    "    bbox_center_y_m = bbox_center_y_px * row['gsd']\n",
    "\n",
    "    # Calculate the distance from the center of the image in meters\n",
    "    distance_m = ((row['center_x_m'] - bbox_center_x_m)**2 + (row['center_y_m'] - bbox_center_y_m)**2)**0.5\n",
    "\n",
    "    return distance_m\n",
    "\n",
    "# Apply the function to the merged dataframe\n",
    "merged_df['distance_from_center'] = merged_df.apply(calculate_distance_from_center, axis=1)\n",
    "\n",
    "# Export the modified dataframe to CSV\n",
    "savepath = \"E:\\\\poweranalysis\\\\data\\\\circles\\\\\"  # Desired output directory\n",
    "merged_df.to_csv(savepath + \"BdA2023_PowerAnalysisData2.csv\", index=False)  # Desired filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>altitude</th>\n",
       "      <th>gsd</th>\n",
       "      <th>transect_id</th>\n",
       "      <th>flight_id</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20220110_Bernardo_0898.jpg</td>\n",
       "      <td>40</td>\n",
       "      <td>0.009040</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bernardo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20220110_Bernardo_0900.jpg</td>\n",
       "      <td>40</td>\n",
       "      <td>0.009040</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bernardo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20220110_Bernardo_0902.jpg</td>\n",
       "      <td>40</td>\n",
       "      <td>0.009040</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bernardo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20220110_Bernardo_0904.jpg</td>\n",
       "      <td>39</td>\n",
       "      <td>0.008814</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bernardo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20220110_Bernardo_0906.jpg</td>\n",
       "      <td>39</td>\n",
       "      <td>0.008814</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bernardo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     filename  altitude       gsd  transect_id  flight_id   \n",
       "0  20220110_Bernardo_0898.jpg        40  0.009040            1          1  \\\n",
       "1  20220110_Bernardo_0900.jpg        40  0.009040            1          1   \n",
       "2  20220110_Bernardo_0902.jpg        40  0.009040            1          1   \n",
       "3  20220110_Bernardo_0904.jpg        39  0.008814            1          1   \n",
       "4  20220110_Bernardo_0906.jpg        39  0.008814            1          1   \n",
       "\n",
       "       site  \n",
       "0  Bernardo  \n",
       "1  Bernardo  \n",
       "2  Bernardo  \n",
       "3  Bernardo  \n",
       "4  Bernardo  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#5a- Quick fix: GSD is in cm instead of m and you forgot\n",
    "\n",
    "df_resolution = pd.read_csv(\"E:\\\\poweranalysis\\\\data\\\\circles\\\\StateLands2022_ind_imgs.csv\") # output from step 2\n",
    "\n",
    "df_resolution['gsd'] = df_resolution['gsd']/100\n",
    "\n",
    "df_resolution.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>agl</th>\n",
       "      <th>transect_id</th>\n",
       "      <th>gsd</th>\n",
       "      <th>land</th>\n",
       "      <th>unit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BdA_6s_Corridors_Flight_01_00002.jpg</td>\n",
       "      <td>33.852027</td>\n",
       "      <td>-106.855598</td>\n",
       "      <td>1482.361328</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012945</td>\n",
       "      <td>yes</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BdA_6s_Corridors_Flight_01_00004.jpg</td>\n",
       "      <td>33.852663</td>\n",
       "      <td>-106.855706</td>\n",
       "      <td>1481.988403</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012897</td>\n",
       "      <td>yes</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BdA_6s_Corridors_Flight_01_00006.jpg</td>\n",
       "      <td>33.853300</td>\n",
       "      <td>-106.855819</td>\n",
       "      <td>1482.775757</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012999</td>\n",
       "      <td>yes</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BdA_6s_Corridors_Flight_01_00008.jpg</td>\n",
       "      <td>33.853933</td>\n",
       "      <td>-106.855929</td>\n",
       "      <td>1484.390869</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013207</td>\n",
       "      <td>yes</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BdA_6s_Corridors_Flight_01_00010.jpg</td>\n",
       "      <td>33.854567</td>\n",
       "      <td>-106.856035</td>\n",
       "      <td>1482.853027</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013009</td>\n",
       "      <td>yes</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               filename        lat        long          agl   \n",
       "0  BdA_6s_Corridors_Flight_01_00002.jpg  33.852027 -106.855598  1482.361328  \\\n",
       "1  BdA_6s_Corridors_Flight_01_00004.jpg  33.852663 -106.855706  1481.988403   \n",
       "2  BdA_6s_Corridors_Flight_01_00006.jpg  33.853300 -106.855819  1482.775757   \n",
       "3  BdA_6s_Corridors_Flight_01_00008.jpg  33.853933 -106.855929  1484.390869   \n",
       "4  BdA_6s_Corridors_Flight_01_00010.jpg  33.854567 -106.856035  1482.853027   \n",
       "\n",
       "   transect_id       gsd land unit  \n",
       "0            1  0.012945  yes    6  \n",
       "1            1  0.012897  yes    6  \n",
       "2            1  0.012999  yes    6  \n",
       "3            1  0.013207  yes    6  \n",
       "4            1  0.013009  yes    6  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5b- Quick fix, if detections and independent image info isn't matching\n",
    "df_resolution = pd.read_csv(\"E:\\\\poweranalysis\\\\data\\\\circles\\\\BdA_2023_ind_imgs.csv\") # output from step 2\n",
    "def lowercase_extension(filename):\n",
    "    name, ext = filename.rsplit('.', 1)\n",
    "    return f'{name}.{ext.lower()}'\n",
    "\n",
    "# Lowercase the extension part of filenames in one of the dataframes\n",
    "df_resolution['filename'] = df_resolution['filename'].apply(lowercase_extension)\n",
    "df_resolution.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW: Attempts on the process to use projected images for this process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent tile paths have been copied to: E:\\poweranalysis\\18a03_tiles\n"
     ]
    }
   ],
   "source": [
    "#Derive independent (ie, non-overlapping) projected image footprints (whole image-- not the detection radius)\n",
    "#This script works\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from osgeo import gdal, osr\n",
    "\n",
    "def calculate_overlap(tile1_path, tile2_path):\n",
    "    # Open the raster datasets\n",
    "    tile1_dataset = gdal.Open(tile1_path)\n",
    "    tile2_dataset = gdal.Open(tile2_path)\n",
    "\n",
    "    # Get the geotransform information (bounding box) of the tiles\n",
    "    geotransform1 = tile1_dataset.GetGeoTransform()\n",
    "    geotransform2 = tile2_dataset.GetGeoTransform()\n",
    "\n",
    "    # Calculate overlapping area\n",
    "    x_min1, y_max1 = geotransform1[0], geotransform1[3]\n",
    "    x_max1, y_min1 = x_min1 + geotransform1[1] * tile1_dataset.RasterXSize, y_max1 + geotransform1[5] * tile1_dataset.RasterYSize\n",
    "\n",
    "    x_min2, y_max2 = geotransform2[0], geotransform2[3]\n",
    "    x_max2, y_min2 = x_min2 + geotransform2[1] * tile2_dataset.RasterXSize, y_max2 + geotransform2[5] * tile2_dataset.RasterYSize\n",
    "\n",
    "    x_overlap = max(0, min(x_max1, x_max2) - max(x_min1, x_min2))\n",
    "    y_overlap = max(0, min(y_max1, y_max2) - max(y_min1, y_min2))\n",
    "\n",
    "    return x_overlap * y_overlap\n",
    "\n",
    "def is_overlapping(tile_path, independent_tiles):\n",
    "    for independent_tile_path in independent_tiles:\n",
    "        if calculate_overlap(tile_path, independent_tile_path) > 0:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_non_overlapping_tiles(tile_directory):\n",
    "    all_tile_paths = [os.path.join(tile_directory, file) for file in os.listdir(tile_directory) if file.endswith(\".tif\")]\n",
    "    independent_tile_paths = [all_tile_paths[0]]\n",
    "\n",
    "    for tile_path in all_tile_paths[1:]:\n",
    "        if not is_overlapping(tile_path, independent_tile_paths):\n",
    "            independent_tile_paths.append(tile_path)\n",
    "\n",
    "    return independent_tile_paths\n",
    "\n",
    "tile_directory = \"E:\\\\poweranalysis\\\\data\\\\18a03\"\n",
    "independent_tile_paths = find_non_overlapping_tiles(tile_directory)\n",
    "\n",
    "# Create a new directory for independent tiles\n",
    "output_directory = \"E:\\\\poweranalysis\\\\18a03_tiles\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Copy independent tiles to the output directory\n",
    "for tile_path in independent_tile_paths:\n",
    "    tile_filename = os.path.basename(tile_path)\n",
    "    output_path = os.path.join(output_directory, tile_filename)\n",
    "    shutil.copy(tile_path, output_path)\n",
    "\n",
    "print(\"Independent tile paths have been copied to:\", output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translate image coordinates of SAHI detections to geographic ones \n",
    "#This still produces incorrect values :/\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json  # Import the json module\n",
    "from osgeo import gdal, osr\n",
    "\n",
    "# Directory containing the GeoTIFF rasters\n",
    "raster_dir = \"E:\\\\poweranalysis\\\\data\\\\12c01_tiles\\\\\"\n",
    "\n",
    "# Define a function to convert image coordinates to geographic coordinates\n",
    "def image_to_geographic_coordinates(image_x, image_y, geo_transform):\n",
    "    x = geo_transform[0] + image_x * geo_transform[1]\n",
    "    y = geo_transform[3] + image_y * geo_transform[5]\n",
    "    return x, y\n",
    "\n",
    "# Read the CSV file with bounding boxes\n",
    "csv_file = \"E:\\\\poweranalysis\\\\data\\\\12c01_coco_detections.csv\"\n",
    "output_csv_file = \"E:\\\\poweranalysis\\\\data\\\\12c01_coco_detections_dist.csv\"\n",
    "\n",
    "with open(csv_file, \"r\") as file, open(output_csv_file, \"w\", newline='') as output_file:\n",
    "    reader = csv.DictReader(file)\n",
    "    fieldnames = reader.fieldnames + [\"geo_x\", \"geo_y\", \"dist_m\"]\n",
    "    writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        image_id = row[\"image_id\"]\n",
    "        raster_filename = os.path.join(raster_dir, f\"{image_id}\")  # Construct raster filename from directory and image ID\n",
    "\n",
    "        if not os.path.exists(raster_filename):\n",
    "            print(f\"Warning: Raster file '{raster_filename}' does not exist for Image ID '{image_id}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load the GeoTIFF raster\n",
    "        ds = gdal.Open(raster_filename)\n",
    "\n",
    "        # Extract bounding box coordinates from the \"bbox\" column\n",
    "        bbox_str = row[\"bbox\"]\n",
    "        bbox = json.loads(bbox_str)\n",
    "        xmin, ymin, width, height = map(float, bbox)\n",
    "\n",
    "        # Calculate the center of the bounding box in image coordinates\n",
    "        center_x = xmin + (width / 2)\n",
    "        center_y = ymin + (height / 2)\n",
    "\n",
    "        # Calculate the geographic coordinates of the pixel corresponding to the bounding box center\n",
    "        geo_transform = ds.GetGeoTransform()\n",
    "        geo_x, geo_y = image_to_geographic_coordinates(center_x, center_y, geo_transform)\n",
    "\n",
    "        # Calculate the distance between the geographic center and raster center\n",
    "        raster_center_x = geo_transform[0] + (ds.RasterXSize / 2) * geo_transform[1]\n",
    "        raster_center_y = geo_transform[3] + (ds.RasterYSize / 2) * geo_transform[5]\n",
    "        distance = np.sqrt((geo_x - raster_center_x) ** 2 + (geo_y - raster_center_y) ** 2)\n",
    "\n",
    "        # Add the new columns to the row\n",
    "        row[\"geo_x\"] = geo_x\n",
    "        row[\"geo_y\"] = geo_y\n",
    "        row[\"dist_m\"] = distance\n",
    "\n",
    "        # Write the updated row to the new CSV\n",
    "        writer.writerow(row)\n",
    "\n",
    "        # Close the GeoTIFF dataset\n",
    "        ds = None\n",
    "\n",
    "# Optionally, you can rename the new CSV file to the original filename if desired\n",
    "if os.path.exists(csv_file):\n",
    "    os.remove(csv_file)  # Remove the original file if it exists\n",
    "os.rename(output_csv_file, csv_file)  # Rename the new CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. Output saved to E:\\poweranalysis\\data\\output.csv\n"
     ]
    }
   ],
   "source": [
    "#Translate outputs of SAHI detections to conform to the former YOLOv3 format used in Sa'doun's projection script\n",
    "#This works\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Input CSV file path\n",
    "input_csv_file = \"E:\\\\poweranalysis\\\\data\\\\12c01_unproj_coco_detections.csv\"\n",
    "\n",
    "# Output CSV file path\n",
    "output_csv_file = \"E:\\\\poweranalysis\\\\data\\\\output.csv\"\n",
    "\n",
    "# Directory where the images are located\n",
    "image_directory = \"E:\\\\poweranalysis\\\\data\\\\circles\\\\12c01_subset\\\\\"\n",
    "\n",
    "# Read the input CSV file into a DataFrame\n",
    "df = pd.read_csv(input_csv_file)\n",
    "\n",
    "# Define a function to parse the \"bbox\" column\n",
    "def parse_bbox(bbox_str):\n",
    "    bbox = eval(bbox_str)\n",
    "    xmin, ymin, width, height = bbox\n",
    "    xmax = xmin + width\n",
    "    ymax = ymin + height\n",
    "    return xmin, ymin, xmax, ymax\n",
    "\n",
    "# Apply the parsing function to the \"bbox\" column\n",
    "df[\"xmin\"], df[\"ymin\"], df[\"xmax\"], df[\"ymax\"] = zip(*df[\"bbox\"].apply(parse_bbox))\n",
    "\n",
    "# Derive the \"image_path\" by adding \"image_id\" to the image directory\n",
    "df[\"image_path\"] = image_directory + df[\"image_id\"]\n",
    "\n",
    "# Use \"category_id\" for \"label\" and \"score\" for \"confidence\"\n",
    "df.rename(columns={\"category_id\": \"label\", \"score\": \"confidence\"}, inplace=True)\n",
    "\n",
    "# Extract \"x_size\" and \"y_size\" by opening each image\n",
    "def get_image_size(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        return img.size\n",
    "\n",
    "df[\"x_size\"], df[\"y_size\"] = zip(*df[\"image_path\"].apply(get_image_size))\n",
    "\n",
    "# Create the output DataFrame with the desired columns\n",
    "output_df = df[[\"image_id\", \"image_path\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"label\", \"confidence\", \"x_size\", \"y_size\"]]\n",
    "output_df.columns = [\"image\", \"image_path\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"label\", \"confidence\", \"x_size\", \"y_size\"]\n",
    "\n",
    "# Save the output DataFrame to a new CSV file\n",
    "output_df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "print(f\"Conversion complete. Output saved to {output_csv_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
